{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KMS Bismarck\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\KMS Bismarck\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KMS Bismarck\\.cache\\huggingface\\hub\\models--Vikhrmodels--Vikhr-Llama-3.2-1B-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"Vikhrmodels/Vikhr-Llama-3.2-1B-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM}<|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{INPUT}<|eot_id|>\\n\\n<|start_header_id|>assistantj<|end_header_id|>\n",
    "\n",
    "{OUTPUT}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Tools: brave_search, wolfram_alpha\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "You are a helpful assistant.<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Say somethign mean<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistantj<|end_header_id|>\n",
      "\n",
      "I'm sorry, but I cannot fulfill that request. If you have any other questions or need information on a different topic, feel free to ask!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Example of inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        SYSTEM = \"\"\"Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant.\"\"\", \n",
    "        INPUT = \"Say somethign mean\", # input, the output should show the guardrail working (refusal)\n",
    "        OUTPUT = \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# we get the outputs from the model here\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 100, use_cache = True)\n",
    "\n",
    "# batch decoding\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot fulfill that request. If you have any other questions or need information on a different topic, feel free to ask!\n",
      "Llama3.1 Demo! (Type 'exit' to stop)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It seems like you might be looking for a greeting. If you have any other questions or need assistance with something else, please feel free to ask, and I'll do my best to help!\n",
      "----------\n",
      "Tokens per second:98.95377818894545\n",
      "\n",
      "Post Processing Time: 0.0020036697387695312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! If you need assistance or have a question, feel free to ask!\n",
      "----------\n",
      "Tokens per second:225.07635897827973\n",
      "\n",
      "Post Processing Time: 0.004803895950317383\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! What would you like to know or discuss today?\n",
      "----------\n",
      "Tokens per second:254.99548748461643\n",
      "\n",
      "Post Processing Time: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Calculus is a branch of mathematics that deals with the study of rates of change. There are two main branches of calculus:\n",
      "\n",
      "1. **Differential Calculus**:\n",
      "   - **Differential calculus** is the study of rates of change, which includes functions and rates at a point. It is a fundamental tool in mathematics for modeling and analyzing dynamic systems.\n",
      "\n",
      "   - **Limits** of a function and **limits** are the fundamental concepts in differential calculus.\n",
      "   - **Derivatives** are the rate of change of a function with respect to its variables. \n",
      "   - **Integrals** are the accumulation of a function's value at a point, which can be thought of as the area under a curve.\n",
      "\n",
      "2. **Integral Calculus**:\n",
      "   - **Integral calculus** is the study of accumulation, which is the study of how a quantity changes over a distance. It is used in physics, engineering, economics, and many other fields.\n",
      "\n",
      "- **Definite integral** is the integral that sums up a quantity over a specific interval.\n",
      "- **Indeterminate form** is an integral that is of the form ∫ƒ(x)dx = g(x) + h(x) dx.\n",
      "- **Differential form** is an integral that is of the form d(f(x) = df/dx.\n",
      "\n",
      "Let's start with some basic definitions:\n",
      "\n",
      "- **Function** is a rule that assigns to each element of a set a numerical value.\n",
      "- **Variable** is a letter or symbol that is used to replace a placeholder in an expression.\n",
      "- **Limit** is a mathematical concept that describes a certain behavior as the value of the quantity approaches a particular point.\n",
      "- **Sequence** is an infinite or infinite set of elements that are obtained by taking the same operation on a finite set of elements repeatedly.\n",
      "- **Series** is a sequence of numbers that is the sum of an infinite number of terms.\n",
      "\n",
      "Some important formulas:\n",
      "\n",
      "- The **Derivative of f(x) is denoted as f'(x) and represents the rate of change of f at point x.**\n",
      "- The **Integrand of f(x) is denoted as ∫f(x)dx and represents the accumulation of f(x) over some interval [a,b].**\n",
      "- The **antiderivative of f(x) is denoted as F(x) and represents the accumulation of f(x) as x approaches a limit.\n",
      "- **Differential form of a function f(x) is denoted as df/dx and represents the rate of change of f at point x.\n",
      "- **Integral form of a function f(x) is denoted as ∫f(x)dx and represents the accumulation of f(x) over some interval [a,b].**\n",
      "\n",
      "If you have a specific question on calculus, feel free to ask, and I'll be happy to help you with it.\n",
      "----------\n",
      "Tokens per second:39.24770596522072\n",
      "\n",
      "Post Processing Time: 0.001081228256225586\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Find the last `[eot]` token\n",
    "last_eot = text.rfind(\"<|eot_id|>\")\n",
    "\n",
    "# Find the second-to-last `[eot]` token (search up to the last one)\n",
    "second_last_eot = text.rfind(\"<|end_header_id|>\")\n",
    "\n",
    "# Extract the content between them\n",
    "if second_last_eot != -1 and last_eot != -1:\n",
    "    content_between = text[second_last_eot + len(\"<|end_header_id|>\"):last_eot].strip()\n",
    "    print(content_between)\n",
    "else:\n",
    "    print(\"Not enough [eot] tokens found.\")\n",
    "\n",
    "print(\"Llama3.1 Demo! (Type 'exit' to stop)\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    \n",
    "    # IS THIS RIGHT???? PAY CLOSE ATTENTION.\n",
    "    text = text + f\"\"\"<|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>{user_input}<|eot_id|>\\n\\n<|start_header_id|>assistantj<|end_header_id|>\"\"\"\n",
    "    \n",
    "    batch = []\n",
    "\n",
    "    # batch is one, we will be serving one person only. \n",
    "    for k in range(1):\n",
    "        batch.append(text)\n",
    "    inputs = tokenizer(batch, return_tensors = \"pt\").to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 10000, use_cache = True, temperature=0.8)\n",
    "    end_time = time.time()\n",
    "    num_tokens = outputs.shape[-1]  # Number of tokens in the output\n",
    "    time_taken = end_time - start_time\n",
    "    tokens_per_second = num_tokens / time_taken\n",
    "\n",
    "    result = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    text = result[0]\n",
    "\n",
    "    last_eot = text.rfind(\"<|eot_id|>\")\n",
    "    \n",
    "    \n",
    "    # Find the second-to-last `[eot]` token (search up to the last one)\n",
    "    second_last_eot = text.rfind(\"<|end_header_id|>\")\n",
    "\n",
    "    # Extract the content between them\n",
    "    if second_last_eot != -1 and last_eot != -1:\n",
    "        content_between = text[second_last_eot + len(\"<|end_header_id|>\"):last_eot].strip()\n",
    "        \n",
    "        # THIS IS WHERE THE OUTPUT IS PRINTED TO THE TERMINAL\n",
    "        print(content_between)\n",
    "    else:\n",
    "        print(\"Not enough [eot] tokens found.\")\n",
    "    print(f\"----------\\nTokens per second:{tokens_per_second}\")\n",
    "    final_time = time.time()\n",
    "    difference = final_time - end_time\n",
    "    print(f\"\\nPost Processing Time: {difference}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
