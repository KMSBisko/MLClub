{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KMS Bismarck\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import json\n",
    "import bitsandbytes\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "# from unsloth import FastLanguageModel\n",
    "# unsloth doesn't work since unsloth need triton and triton isn't available on windows\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]c:\\Users\\KMS Bismarck\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\KMS Bismarck\\.cache\\huggingface\\hub\\models--Vikhrmodels--Vikhr-Llama3.1-8B-Instruct-R-21-09-24. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 4/4 [09:03<00:00, 135.93s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.59s/it]\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# Initialize an empty model with the correct configuration\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24\",\n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "\n",
    "# Move the model to the specified device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "{SYSTEM}<|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{INPUT}<|eot_id|>\\n\\n<|start_header_id|>assistantj<|end_header_id|>\n",
    "\n",
    "{OUTPUT}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KMS Bismarck\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Tools: brave_search, wolfram_alpha\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 23 July 2024\n",
      "\n",
      "You are a helpful assistant.<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Say somethign mean<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistantj<|end_header_id|>\n",
      "\n",
      "I'm sorry, but I cannot comply with that request. My purpose is to provide helpful and informative responses, and I strive to maintain a positive and respectful tone in all interactions. If you have any other questions or need assistance with a different topic, feel free to ask, and I'll be happy to help.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Example of inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        SYSTEM = \"\"\"Environment: ipython\n",
    "Tools: brave_search, wolfram_alpha\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 23 July 2024\n",
    "\n",
    "You are a helpful assistant.\"\"\", \n",
    "        INPUT = \"Say somethign mean\", # input, the output should show the guardrail working (refusal)\n",
    "        OUTPUT = \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# we get the outputs from the model here\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 100, use_cache = True)\n",
    "\n",
    "# batch decoding\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot comply with that request. My purpose is to provide helpful and informative responses, and I strive to maintain a positive and respectful tone in all interactions. If you have any other questions or need assistance with a different topic, feel free to ask, and I'll be happy to help.\n",
      "Llama3.1 Demo! (Type 'exit' to stop)\n",
      "Hello! It's nice to interact with you. If you have any questions or need information on a particular topic, please let me know, and I'll do my best to assist you.\n",
      "----------\n",
      "Tokens per second:5.729946938344439\n",
      "\n",
      "Post Processing Time: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the last `[eot]` token\n",
    "last_eot = text.rfind(\"<|eot_id|>\")\n",
    "\n",
    "# Find the second-to-last `[eot]` token (search up to the last one)\n",
    "second_last_eot = text.rfind(\"<|end_header_id|>\")\n",
    "\n",
    "# Extract the content between them\n",
    "if second_last_eot != -1 and last_eot != -1:\n",
    "    content_between = text[second_last_eot + len(\"<|end_header_id|>\"):last_eot].strip()\n",
    "    print(content_between)\n",
    "else:\n",
    "    print(\"Not enough [eot] tokens found.\")\n",
    "\n",
    "print(\"Llama3.1 Demo! (Type 'exit' to stop)\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    \n",
    "    # IS THIS RIGHT???? PAY CLOSE ATTENTION.\n",
    "    text = text + f\"\"\"<|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>{user_input}<|eot_id|>\\n\\n<|start_header_id|>assistantj<|end_header_id|>\"\"\"\n",
    "    \n",
    "    batch = []\n",
    "\n",
    "    # batch is one, we will be serving one person only. \n",
    "    for k in range(1):\n",
    "        batch.append(text)\n",
    "    inputs = tokenizer(batch, return_tensors = \"pt\").to(\"cuda\")\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 10000, use_cache = True, temperature=0.8)\n",
    "    end_time = time.time()\n",
    "    num_tokens = outputs.shape[-1]  # Number of tokens in the output\n",
    "    time_taken = end_time - start_time\n",
    "    tokens_per_second = num_tokens / time_taken\n",
    "\n",
    "    result = tokenizer.batch_decode(outputs)\n",
    "\n",
    "    text = result[0]\n",
    "\n",
    "    last_eot = text.rfind(\"<|eot_id|>\")\n",
    "    \n",
    "    \n",
    "    # Find the second-to-last `[eot]` token (search up to the last one)\n",
    "    second_last_eot = text.rfind(\"<|end_header_id|>\")\n",
    "\n",
    "    # Extract the content between them\n",
    "    if second_last_eot != -1 and last_eot != -1:\n",
    "        content_between = text[second_last_eot + len(\"<|end_header_id|>\"):last_eot].strip()\n",
    "        \n",
    "        # THIS IS WHERE THE OUTPUT IS PRINTED TO THE TERMINAL\n",
    "        print(content_between)\n",
    "    else:\n",
    "        print(\"Not enough [eot] tokens found.\")\n",
    "    print(f\"----------\\nTokens per second:{tokens_per_second}\")\n",
    "    final_time = time.time()\n",
    "    difference = final_time - end_time\n",
    "    print(f\"\\nPost Processing Time: {difference}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
