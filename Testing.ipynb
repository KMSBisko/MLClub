{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import json\n",
    "import bitsandbytes\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "# from unsloth import FastLanguageModel\n",
    "# unslot doesn't work since unsloth need triton and triton isn't available on windows\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f643048e5c4430ca76ca4f713799d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a BitsAndBytesConfig object with 8-bit loading\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# Initialize an empty model with the correct configuration\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24\",\n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "\n",
    "# Load the model with disk offloading\n",
    "# model = load_checkpoint_and_dispatch(\n",
    "#     model,\n",
    "#     \"Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24\",\n",
    "#     device_map=\"auto\",\n",
    "#     offload_folder=\"D:\\mtrie\\OneDrive\\Downloads\\MLClub\\offload\"  # Specify the folder for disk offloading\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello llama modelers! I am excited to share with you my latest creation, the Peruvian Long-necked Llama. This breed is known for its distinctive long neck and is a favorite among llama enthusiasts. So, let's dive into the details of this model and see how it can be used to enhance your knowledge of llama anatomy.\n",
      "### Introduction to the Peruvian Long-necked Llama\n",
      "The Peruvian Long-necked Llama is a breed that has been developed over the centuries for its unique characteristics.\n"
     ]
    }
   ],
   "source": [
    "# Example input_ids (replace with actual input data)\n",
    "input_text = \"Hello llama model\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Ensure inputs are on the same device as the model\n",
    "inputs = input_ids.to(next(model.parameters()).device)\n",
    "\n",
    "# Generate outputs from the model\n",
    "outputs = model.generate(inputs, max_new_tokens=100, use_cache=True)\n",
    "\n",
    "# Batch decoding\n",
    "text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello there! as you may know, i'm passionate about helping people achieve their goals and live a happy and healthy lifestyle. my expertise includes nutrition, fitness, and mindfulness, and i'm here to support you every step of the way.\n",
      "\n",
      "### Step-by-Step Guide to Achieving Your Goals\n",
      "\n",
      "#### Step 1: Define Your Goals\n",
      "- **Reflect on what you want to achieve.** Is it to lose weight, improve your health, or increase your energy levels?\n",
      "- **Make your goals\n",
      "Not enough [eot] tokens found.\n",
      "----------\n",
      "Tokens per second: 1.0445257847068814\n",
      "\n",
      "Post Processing Time: 0.026718616485595703\n",
      "\n",
      "hi, I've been trying to get a new job and have been applying to a variety of positions. However, I've been getting a lot of rejection letters. It's been really frustrating, and I'm starting to feel discouraged. Here are some steps you can take to help you stay motivated and keep your job search on track:\n",
      "\n",
      "1. **Reflect on Your Experience**: Take some time to review your past work experiences and skills. This can help you identify areas where you can improve and develop a stronger\n",
      "Not enough [eot] tokens found.\n",
      "----------\n",
      "Tokens per second: 0.7715150723407423\n",
      "\n",
      "Post Processing Time: 0.024719715118408203\n",
      "\n",
      "say a riddle is a riddle because it is\n",
      "riddle-like. But what is it to be riddle-like?\n",
      "### Introduction\n",
      "The riddle is a form of wordplay that has been a staple of language and culture for centuries. It is a puzzle wrapped in an enigma, which presents a problem that requires thought and sometimes a clever solution. Riddles are often used to challenge one's thinking and to exercise one's wit. They can be simple or complex, and they can be found in literature\n",
      "Not enough [eot] tokens found.\n",
      "----------\n",
      "Tokens per second: 0.909345103764651\n",
      "\n",
      "Post Processing Time: 0.018009185791015625\n",
      "\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import time  # Import the time module\n",
    "\n",
    "text = \"\"  # Initialize text before the loop\n",
    "\n",
    "while True:\n",
    "    input_text = input(\"Enter input text: \")\n",
    "    if input_text.lower() == 'exit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    text = input_text\n",
    "    print(f\"Enter input text: {input_text}\")\n",
    "    \n",
    "    batch = []\n",
    "\n",
    "    # batch is one, we will be serving one person only. \n",
    "    for k in range(1):\n",
    "        batch.append(text)\n",
    "\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Ensure inputs are on the same device as the model\n",
    "    inputs = input_ids.to(next(model.parameters()).device)\n",
    "    start_time = time.time()\n",
    "    # Generate outputs from the model\n",
    "    outputs = model.generate(inputs, max_new_tokens=100, use_cache=True) # can set temperature \n",
    "    end_time = time.time()\n",
    "    # Batch decoding\n",
    "    text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(text)\n",
    "\n",
    "    num_tokens = outputs.shape[-1]  # Number of tokens in the output\n",
    "    time_taken = end_time - start_time\n",
    "    tokens_per_second = num_tokens / time_taken\n",
    "\n",
    "    last_eot = text.rfind(\"<|eot_id|>\")\n",
    "\n",
    "    # Find the second-to-last `[eot]` token (search up to the last one)\n",
    "    second_last_eot = text.rfind(\"<|end_header_id|>\")\n",
    "\n",
    "    # Extract the content between them\n",
    "    if second_last_eot != -1 and last_eot != -1:\n",
    "        content_between = text[second_last_eot + len(\"<|end_header_id|>\"):last_eot].strip()\n",
    "\n",
    "        # THIS IS WHERE THE OUTPUT IS PRINTED TO THE TERMINAL\n",
    "        print(content_between)\n",
    "    else:\n",
    "        print(\"Not enough [eot] tokens found.\")\n",
    "    print(f\"----------\\nTokens per second: {tokens_per_second}\")\n",
    "    final_time = time.time()\n",
    "    difference = final_time - end_time\n",
    "    print(f\"\\nPost Processing Time: {difference}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
